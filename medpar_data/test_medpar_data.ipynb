{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test EventStreamGPT on Medpar data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create esgpt environment:\n",
    "\n",
    "1. mamba create --name esgpt python=3.10\n",
    "2. mamba activate esgpt\n",
    "3. pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rootutils\n",
    "\n",
    "root = rootutils.setup_root(os.path.abspath(''), dotenv=True, pythonpath=True, cwd=True)\n",
    "\n",
    "import polars as pl\n",
    "pl.Config.set_tbl_cols(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denom_yyyy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal, create mbsf_medpar_denom by creating the symlinks found in the medpar_data/README.md.\n",
    "\n",
    "This file contains per-subject data. It has one row per subject, with each row containing a subject identifier (here called \"`bene_id`\"),a date of birth, state, sex, race, and other information printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet('medpar_data/mbsf_medpar_denom/denom_2000.parquet')\n",
    "print(\"Dynamic Measurement Columns:\\n  * \" + '\\n  * '.join(df.columns))\n",
    "display(df.head(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dod column:\n",
    "print(\"dod column:\")\n",
    "print(df.select('dod').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small EDA to assess the temporalities of each variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can a bene_id have multiple race values?\n",
    "# Load the DataFrame lazily\n",
    "df = pl.scan_parquet('medpar_data/mbsf_medpar_denom/denom_2000.parquet')\n",
    "\n",
    "# Group by 'bene_id', count unique 'race' codes, and filter for those with more than one unique code\n",
    "multiple_races = (\n",
    "    df\n",
    "    .groupby(\"bene_id\")\n",
    "    .agg(pl.col(\"race\").n_unique().alias(\"unique_race_count\"))\n",
    "    .filter(pl.col(\"unique_race_count\") > 1)\n",
    ").head(5)\n",
    "\n",
    "# Collect the results to view\n",
    "result = multiple_races.collect()\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, a bene_id can have only one race value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values of zcta per bene_id\n",
    "# Load the DataFrame lazily\n",
    "df = pl.scan_parquet('medpar_data/mbsf_medpar_denom/denom_2000.parquet')\n",
    "\n",
    "# Group by 'bene_id', count unique 'zcta' codes, and filter for those with more than one unique code\n",
    "multiple_zcta = (\n",
    "    df\n",
    "    .groupby(\"bene_id\")\n",
    "    .agg(pl.col(\"zcta\").n_unique().alias(\"unique_zcta_count\"))\n",
    "    .filter(pl.col(\"unique_zcta_count\") > 1)\n",
    ").head(5)\n",
    "\n",
    "# Collect the results to view\n",
    "result = multiple_zcta.collect()\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here zcta in one year can be considered as static. But thorough the years, it would probablily be time dependent ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inpatient_yyyy.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains dynamic data quantifying both fictional subject hospital admissions, and diagnoses measured for those subjects. Each row of this file records a unique diagnoses measurement for a patient, affiliated with the associated admission listed in the row. This means that admission level information is _heavily duplicated_ within this file, which is a phenomena sometimes observed in real data, and something we'll need to account for in our pipeline's setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.scan_parquet('medpar_data/mbsf_medpar_denom/inpatient_2000.parquet')\n",
    "print(\"Dynamic Measurement Columns:\\n  * \" + '\\n  * '.join(df.columns))\n",
    "display(df.head(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pl.read_parquet('medpar_data/mbsf_medpar_denom/inpatient_2000.parquet').select([\n",
    "    'admission_date',\n",
    "    'discharge_date'\n",
    "]).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row :ts_col is used for data-sources where each row represents one event, and start_/end_ts_col for data-sources where each row specifies a range in time.\n",
    "\n",
    "So range in time is the admission and discharge date.\n",
    "One event would be the primary diag for example ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print admsn_type_cd\n",
    "display(pl.read_parquet('medpar_data/mbsf_medpar_denom/inpatient_2000.parquet').select([\n",
    "    'dschrgcd',\n",
    "    'admsn_type_cd',\n",
    "    \"dschrg_dstntn_cd\",\n",
    "    \"primary_diag\",\n",
    "    \"diagnoses\"\n",
    "]).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique discharge code values\n",
    "print(pl.read_parquet('medpar_data/mbsf_medpar_denom/inpatient_2000.parquet').select([\n",
    "    \"primary_diag\"\n",
    "]).n_unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the dob column\n",
    "display(pl.read_parquet('medpar_data/mbsf_medpar_denom/denom_2000.parquet').select('dob').head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display admission date and discharge date\n",
    "display(pl.read_parquet('medpar_data/mbsf_medpar_denom/inpatient_2000.parquet').select([\n",
    "    'admission_date',\n",
    "    'discharge_date'\n",
    "]).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Medpar Data with ESGPT\n",
    "\n",
    "Now that we see the form of this medpar data, we can examine how to process it with Event Stream GPT. From\n",
    "the base directory of the ESGPT repository, we can run the following command:\n",
    "\n",
    "```bash\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    "\t--config-path=\"$(pwd)/sample_data/\" \\\n",
    "\t--config-name=dataset \\\n",
    "\t\"hydra.searchpath=[$(pwd)/configs]\"\n",
    "```\n",
    "\n",
    "Note that this script, like all built-in ESGPT scripts, uses [Hydra](https://hydra.cc/), a configuration file and experiment run-script library. In hydra, all scripts can take as input a set of composable configuration files which can be overwritten via files or via the command line. If you aren't already familiar with Hydra, you should read through some of their examples or tutorials to gain some familiarity with their system.\n",
    "\n",
    "Before we actually run this command, we need to do 2 things:\n",
    "\n",
    "  1. Decide what we _want_ the command to do, conceptually.\n",
    "  2. Understand what we're _telling_ the library to do, via its input arguments.\n",
    "  \n",
    "### What do we _want_ to happen?\n",
    "We can see that our synthetic data has a few different kinds of things happening to these subjects. In the ESGPT data model, we want to organize this data so that we clearly know who our subjects are, quantify when things happen to those subjects, and record in a sparse manner what is happening to those patients. Let's list a few more specific desiderata:\n",
    "\n",
    "  1. We should expect our system to quantify those subjects in our synthetic data that meet our inclusion criteria (which we haven't yet specified).\n",
    "  2. The system should bucket all interactions for subjects into appropriately defined events, across admissions, discharges and diagnoses.\n",
    "  3. The system should learn appropriate categorical vocabularies, numerical outlier detector models, numerical normalization models, for the various measurements we want to extract (which we haven't yet specified).\n",
    "  4. The system should produce \"deep-learning friendly\" representations of these data.\n",
    "\n",
    "A quick tangent -- what do we mean by \"deep-learning friendly\" representations of these data? Well, right now, if we were to try to run these data through _any_ deep-learning system for longitudinal data, we'd need to re-format these data such that it is easy to efficiently (ideally $O(1)$) retrieve all data corresponding to a single subject in an organized timeseries format that we can then efficiently (meaning in a manner requiring minimal GPU memory) pass into a sequential neural network. \n",
    "\n",
    "In the current representation, this retrieval process would not be $O(1)$; instead, if we didn't modify the data's organization at all, for each new MRN, we'd need to select from each data file all those rows with that MRN (each selection being an $O(N)$ operation), and then we would need to subsequently sort all the temporal data by timestamp (another $O(L\\ln(L))$ operation).\n",
    "\n",
    "Similarly, if we use a naive, dense encoding of the data per measurement for our DL representation, this will be very wasteful in terms of GPU memory, as each record will need to occupy memory proportionate to the total number of possible measurements we could observe in our data (e.g., the total number of lab tests, plus the total number of vitals signs, plus the total number of admission departments, etc.). Instead, a sparse encoding should be used.\n",
    "\n",
    "These two properties are exactly what we mean by a \"deep-learning friendly\" representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are several questions posed by these desiderata that we need to answer, such as:\n",
    "\n",
    "  1. What are our inclusion criteria?\n",
    "  2. How should we bucket interactions into events?\n",
    "  3. What measurements do we want to extract?\n",
    "  4. How do we want to define \"outliers\"?\n",
    "  5. How do we define \"appropriate categorical vocabularies\"?\n",
    "  6. How do we want to normalize numerical measurements?\n",
    "  \n",
    "To start us off, let's use the following answers:\n",
    "\n",
    "  1. We'll include all subjects who have at least 3 events, with no other inclusion/exclusion criteria.\n",
    "  2. We'll define an \"event\" to be any interactions happening to a patient within a 1 hour period. We'll bucket these interactions together starting at the earliest event. **QUESTION: how to define an event within what time period ?**\n",
    "  3. Ideally, we'd like to extract _all_ measurements. As we'll see, however, due to a limitation in the current version of ESGPT, we'll extract all measurements except for the patient's **height what could be the issue in our case ?**. In particular, we'll extract the occurrence of admissions, discharges, diagnoses, los_day_cnt, as well as the subject's race, sex, state, zcta, county, the values recorded for discharge (like discharge code), and all claims value **QUESTION: do we need the claims value** ?\n",
    "  4. We'll use a very simple outlier model, that excludes numerical data as outliers if their values exceed 1.5 standard deviations from the mean. This is an extremely aggressive cutoff only suitable for this synthetic data setting.\n",
    "  5. We'll keep any categorical observation as a vocabulary element if it occurs at least 5 times.\n",
    "  6. We'll normalize our numerical observations to have zero mean and unit variance.\n",
    "  \n",
    "### Telling the pipeline what to do: input config\n",
    "Now that we have some basic idea of what we want the pipeline to do, let's examine the input configuration file that we pass to the dataset script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat medpar_data/dataset.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of sections in this file. Firstly, the first three lines ensure this config builds on the defaults provided with the ESGPT library, via Hydra's normal mechanisms. If you aren't familiar with this syntax, check out the [Hydra documentation](https://hydra.cc/docs/1.3/advanced/defaults_list/).\n",
    "\n",
    "Next, there is a section defining some overarching variables and a section defining our input sources. We can see this section details the paths to each of our input files as well as the formatting used for (most of) the timestamps within these files. Note that this section makes use of [Hydra/OmegaConf's Interpolations](https://omegaconf.readthedocs.io/en/2.3_branch/grammar.html#interpolation-strings) to simplify the specification of the file paths used. \n",
    "\n",
    "**Warning**: Two parameters in this section are required: `subject_id_col`, and `cohort_name`. This will be explored in more detail later in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporality\n",
    "\n",
    "How this measure varies in time. If TemporalityType.STATIC, this is a static measurement. If TemporalityType.FUNCTIONAL_TIME_DEPENDENT, then this measurement is a time-dependent measure that varies with time and static data in an analytically computable manner (e.g., age). If TemporalityType.DYNAMIC, then this is a measurement that varies in time in a non-a-priori computable manner.\n",
    "\n",
    "Stores the name of this measurement; also the column in the appropriate internal dataframe (subjects_df, events_df, or dynamic_measurements_df) that will contain this measurement. All measurements will have this set.\n",
    "\n",
    "The ‘column’ linkage has slightly different meanings depending on self.modality:\n",
    "\n",
    "If modality == DataModality.UNIVARIATE_REGRESSION, then this column stores the values associated with this continuous-valued measure.\n",
    "\n",
    "If modality == DataModality.MULTIVARIATE_REGRESSION, then this column stores the keys that dictate the dimensions for which the associated values_column has the values.\n",
    "\n",
    "Otherwise, this column stores the categorical values of this measure.\n",
    "\n",
    "Similarly, it has slightly different meanings depending on self.temporality:\n",
    "\n",
    "If temporality == TemporalityType.STATIC, this is an existent column in the subjects_df dataframe.\n",
    "\n",
    "If temporality == TemporalityType.DYNAMIC, this is an existent column in the dynamic_measurements_df dataframe.\n",
    "\n",
    "Otherwise, (when temporality == TemporalityType.FUNCTIONAL_TIME_DEPENDENT), then this is the name the output-to-be-created column will take in the events_df dataframe.\n",
    "\n",
    "Temporality¶\n",
    "As stated above, measurements can take on one of the following three modes relating to how they vary in time:\n",
    "\n",
    "STATIC: in which case they are unchanging and can be linked uniquely to a subject.\n",
    "\n",
    "FUNCTIONAL_TIME_DEPENDENT: in which case they can be specified in functional form dependent only on static subject data and/or a continuous timestamp.\n",
    "\n",
    "DYNAMIC: in which case they are time-varying, but the manner of this variation cannot be specified in a static functional form as in the case of FUNCTIONAL_TIME_DEPENDENT. Accordingly, these measurements are linked to events in a many to one fashion and are identified via a separate, metadata_id identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Temporality in Medpar \n",
    "\n",
    "**denom_yyyy.parquet**\n",
    "\n",
    "STATIC:\n",
    "\n",
    "- sex: static, single_label_classification. Why ? A bene_id can have only one sex.\n",
    "- race: static, single_label_classification. Why ? A bene_id can have only one race. (not like MIMIC-IV)\n",
    "- ZCTA: static, single_label_classification. Why ? bene_id can have only one zcta code affiliated in the single 2000 year. (is it time-dependent though ? Moving throughout the year ?)\n",
    "- State: static, single_label_classification. \n",
    "- age_dob: static, single_label_classification\n",
    "- dod: dynamic, multi_label_classification (can take either null or more dates, type: datetime[ns]). **Question: is NULL = Zero in that case ?**\n",
    "- \n",
    "\n",
    "FUNCTIONAL TIME DEPENDENT\n",
    "- age: functional_time_dependent measure\n",
    "\n",
    "**inpatient_yyyy.parquet**\n",
    "\n",
    "- bene_id\n",
    "- year\n",
    "- adm_id\n",
    "- admission_date\n",
    "- discharge_date\n",
    "- dschrgcd: dynamic, single_label_classification (codes are string from 0-9)\n",
    "- diagnoses: dynamic, multi_label_classification\n",
    "- los_day_cnt: dynamic, univariate_regression\n",
    "- dschrg_dstntn_cd: dynamic, single_label_classification\n",
    "- src_admsn_cd: dynamic, single_label_classification\n",
    "- admsn_type_cd: dynamic, single_label_classification\n",
    "  * drg_price_amt\n",
    "  * drg_outlier_pmt_amt\n",
    "  * pass_thru_amt\n",
    "  * mdcr_pmt_amt\n",
    "  * bene_blood_ddctbl_amt\n",
    "  * bene_prmry_pyr_amt\n",
    "  * bene_ip_ddctbl_amt\n",
    "  * bene_pta_coinsrnc_amt\n",
    "  * admission_index\n",
    "  * non_external_all\n",
    "- primary_diag: dynamic, multi_label_classification (as MIMIC-IV)\n",
    "  * non_external_primary\n",
    "- multivariate_regression [\"primary_diag\", \"drg_price_amt\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we have a section defining the various measurements we'll exctract in this dataset. We can see we specify each of the measurements we discussed above:\n",
    "  1. sex, race, zcta, state **QUESTION: are state, zcta static?**  is extracted as a static, multiple/single_label_classification measure. \n",
    "  2. `age` is extracted as a `functional_time_dependent` measure, leveraging the date-of-birth column `dob`. _Note that this is where we define the timestamp format for the `dob` column, as it is a timestamp formatted static column!_\n",
    "  3. admsn_type_cd, diagnoses, primary_diag, is extracted as a `dynamic`, `multi_label_classification` measure.\n",
    "  4. `HR`, and `temp` are extracted as `dynamic`, `univariate_regression` measures.\n",
    "  5. `lab_name` and `lab_value` are extracted as a single `dynamic`, `multivariate_regression` measure.\n",
    "  \n",
    "Note that the terms `static`, `functional_time_dependent`, & `dynamic` and `single_label_classification`, `multi_label_classification`, `univariate_regression`, and `multivariate_regression`, are defined enumerations in the `EventStream.data.config` sub-module, and dictate where measurements are stored and how they are pre-processed.\n",
    "  \n",
    "Finally, we have the remaining set of parameters, which define our inclusion-exclusion criteria (by specifying `min_events_per_subject`), our outlier and normalizer model configuration parameters (`normalization` being omitted here as what we want is the default value), our filtering thresholds for vocabulary elements, and the aggregation time-scale for events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What else _could_ we have specified?\n",
    "To better understand the structure of this input specification, let's explore this input configuration file in a bit more detail. To start with, let's look at what the default, base config contains (the config we inherit from in the defaults list):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here they usually define a special event like VISIT when admission_time = discharge_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
    "command = \"\"\"\\\n",
    "PYTHONPATH=$(pwd):$PYTHONPATH ./scripts/build_dataset.py \\\n",
    " --config-path=\"$(pwd)/medpar_data/\" \\\n",
    " --config-name=dataset \\\n",
    " \"hydra.searchpath=[$(pwd)/configs]\" \"\"\"\n",
    "\n",
    "command_out = subprocess.run(command, shell=True, capture_output=True)\n",
    "print(command_out.stdout.decode())\n",
    "\n",
    "if command_out.returncode == 1:\n",
    "    print(\"Command Errored!\")\n",
    "\n",
    "print(command_out.stderr.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh processed_data/CMS_sample/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
